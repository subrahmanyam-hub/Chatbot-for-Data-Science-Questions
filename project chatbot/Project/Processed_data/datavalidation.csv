Keywords,Answers
ensemble learning,ensemble learning methods is a machine learning method that contributes to combining base models to create one efficient predictive model it boosts the overall development of the process 
simple ensemble techniques,the simple ensemble techniques are max voting avearge weighted average
max voting,the max voting method is generally used for classification problems in this technique multiple models are used to make predictions for each data point the predictions by each model are considered as a vote the predictions which we get from the majority of the models are used as the final prediction 
voting classifiers,voting classifiers are usually used in classification problems imagine you trained and fitted a few classifiers logistic regression classifier svm classifiers random forest classifier among others to your training dataset 
weighted average,weighted average is an extension of the averaging method all models are assigned different weights defining the importance of each model for prediction for instance if two of your colleagues are critics while others have no prior experience in this field then the answers by these two friends are given more importance as compared to the other people 
advanced ensemble techniques,the advanced ensemble techniques are stacking blending bagging boosting
stacking,stacking is an ensemble learning technique that uses predictions from multiple models for example decision tree knn or svm to build a new model this model is used for making predictions on the test set below is a stepwise explanation for a simple stacked ensemble 
blending,blending uses only a holdout validation set from the train set to make predictions 
bagging, bagging is combining the results of multiple models for instance all decision trees to get a generalized result different bagging algorithms are bagging metaestimator random forest
bootstrapping,bootstrapping is a sampling technique in which we create subsets of observations from the original dataset with replacement the size of the subsets is the same as the size of the original set 
boosting,boosting is a sequential process where each subsequent model attempts to correct the errors of the previous model the succeeding models are dependent on the previous model different boosting algorithms are adaboost gbm xgbm light gbm catboost
bagging metaestimator,bagging metaestimator is an ensembling algorithm that can be used for both classification baggingclassifier and regression baggingregressor problems it follows the typical bagging technique to make predictions following are the steps for the bagging metaestimator algorithm 
random forest,random forest is another ensemble machine learning algorithm that follows the bagging technique it is an extension of the bagging estimator algorithm the base estimators in random forest are decision trees 
adaboost,adaptive boosting or adaboost is one of the simplest boosting algorithms usually decision trees are used for modelling multiple sequential models are created each correcting the errors from the last model adaboost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly 
gradient boosting,gradient boosting or gbm is another ensemble machine learning algorithm that works for both regression and classification problems gbm uses the boosting technique combining a number of weak learners to form a strong learner regression trees used as a base learner each subsequent tree in series is built on the errors calculated by the previous tree 
xgboost,xgboost extreme gradient boosting is an advanced implementation of the gradient boosting algorithm xgboost has proved to be a highly effective ml algorithm extensively used in machine learning competitions and hackathons xgboost has high predictive power and is almost times faster than the other gradient boosting techniques it also includes a variety of regularization which reduces overfitting and improves overall performance 
light gbm,lightgbm is a gradient boosting framework that uses treebased algorithms and follows leafwise approach while other algorithms work in a levelwise approach pattern the images below will help you understand the difference in a better way 
catboost,catboost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithm 
different parameters used bagging metaestimator,different parameters used in bagging base estimator n estimators max samples max features n jobs
different parameters used random forest,different parameters used in random forest n estimators criterion max features max depth min samples split min samples leaf max leaf nodes n jobs
different parameters used adaboost,different parameters used in adaboost base estimators n estimators learning rate max depth
different parameters used gradient boosting,different parameters used in gradient boosting min samples leaf min weight fraction leaf
different parameters used xgboost,different parameters used in xgboost nthread gamma eta min child weight
different parameters used light gbm,different parameters used in light gbm num iterations num leaves min data in leaf 
different parameters used catboost,different parameters used in catboost loss function iterations learning rate
crossvalidation,crossvalidation is a technique that involves partitioning the original observation dataset into a training set used to train the model and an independent set used to evaluate the analysis 
kfold crossvalidation,in kfold crossvalidation we divide the dataset into k equal parts after this we loop over the entire dataset k times in each iteration of the loop one of the k parts is used for testing and the other k parts are used for training using kfold crossvalidation each one of the k parts of the dataset ends up being used for training and testing purposes 
model evaluation metrics,model evaluation metrics are required to quantify model performance the choice of evaluation metrics depends on a given machine learning task such as classification regression ranking clustering topic modeling among others some metrics such as precisionrecall are useful for multiple tasks supervised learning tasks such as classification and regression constitutes a majority of machine learning applications in this article we focus on metrics for these two supervised learning models 
classification accuracy,classification accuracy is a common evaluation metric for classification problems it is the number of correct predictions made as a ratio of all predictions made we use sklearn module to compute the accuracy of a classification task 
logarithmic loss,logarithmic loss logloss measures the performance of a classification model where the prediction input is a probability value between and log loss increases as the predicted probability diverges from the actual label the goal of machine learning models is to minimize this value as such smaller logloss is better with a perfect model having a log loss of 
area curve auc,area under roc curve is a performance metric for measuring the ability of a binary classifier to discriminate between positive and negative classes 
mean absolute error,mean absolute error can be defined as the difference between the actual values and the predicted values s
mean squared error,mean squared error or commonly known as the mse is the average of the squared errors used as the loss function for the least squares values of regression it is basically the sum of the square of the difference between the predicted target variables and the actual target variables which is finally divided by the number of data points 
rsquared,rsquared can be defined as the statistical measure that is used to represent the goodness fit of a regression model the ideal value of rsquare is known to be closer the value of rsquare to better is the fitted model 
adjusted rsquared,adjusted rsquared is quite different from rsquared as it takes into account the number of independent variables that are used for predicting the target variable in this way one can determine if the addition of these new variables will actually increase the model fit or not 
confusion matrix,a confusion matrix can be defined as the n n matrix that is used to evaluate the performance of a classification model n is the number of target classes the matrix is used to evaluate and compare the actual target values with that of the values predicted by machine model learning 
precision recall,precision is the ratio of true positives and all the positives precision gives us the measure of all the relevant data points recall is termed as the measure of the machine model that correctly identifies the true positives recall gives an estimate of how accurately your model identifies the relevant data 
f score,the fscore can be determined as the harmonic mean of both precision and recall the highest value of fscore can be and the lowest can be 
confidence interval,confidence interval or ci is the range of values which is required to meet a certain confidence level in order to estimate the features of the total population 
gini coefficient,the gini coefficient or gini index is a popular metric for imbalanced class values it is a statistical measure of distribution developed by the italian statistician corrado gini in 
predictive power,predictive power is a synthetic metric which satisfies interesting properties like it is always been and where represents that the feature subset has no predictive power and represents that the feature subset has maximum predictive power and is used to select a good subset of features in any machine learning project 
root mean square error,root mean squared erro or rmse is defined as the measure of the differences between the values predicted by a model and the values actually observed it is basically the square root of mse mean squared error which is the average of the squared error used as the loss function for least squares regression 
resampling methods,resampling methods are used to estimate the precision of the sample statistics exchanging labels on data points and validating models 
correlation analysis,correlation analysis is a statistical method to evaluate the strength of the relationship between two quantitative variables it consists of autocorrelation coefficients estimated and calculated to make a different spatial relationship it is used to correlate data based on distance 
type i error,type i error false positive error it occurs when the null hypothesis is true but it gets rejected meaning it claims something has happened when it hasn t t
type ii error,ype ii error false negative error it occurs when the null hypothesis gets accepted when it s not true meaning it claims nothing happened when something has happened 
test harness,the test harness is the data you will train and test an algorithm against and the performance measure you will use to assess its performance it is important to define your test harness well so that you can focus on evaluating different algorithms and thinking deeply about the problem 
performance measure,the performance measure is the way you want to evaluate a solution to the problem it is the measurement you will make of the predictions made by a trained model on the test dataset performance measures are typically specialized to the class of problem you are working with for example classification regression and clustering 
natural language generation,natural language generation is a part of ai and generates natural language texts from structured data to produce an output it can be seen as nlp s reverse process where nlp is used to understand and interpret the natural language to form data and nlu is used to generate outputs in natural language from structured data 
signal processing,signal processing is a method that enables software to analyze modify and synthesize signals in nlp these can be sound or text signals 
pragmatic analysis ,the pragmatic analysis is the process of information extraction from the given text it is a set of linguistic and logical tools that enable us to churn out the meaning of the given structure of a text 
syntactic analysis ,the syntactic analysis also referred to as parsing and syntax analysis is the phase in which we try to process the given text s structure this process tries to draw meaning from the text by comparing it to formal grammar rules or syntax 
semantic analysis ,the semantic analysis is the process of understanding the meaning of the text in the way humans perceive and communicate it focuses on larger parts of data for processing as compared to other analysis techniques 
sentiment analysis ,the sentiment analysis also known as opinion mining and emotion ai is a process of detecting the polarity of the opinion in the text or can be a part of it it is majorly used to identify extract and quantify user or customer reviews polarity survey responses or social media opinions 
discourse analysis ,discourse is a structured group of the sentence discourse analysis can be termed as an approach to analyzing the discourse i e text or language it involves text interpretations and interactions 
pragmatic ambiguity ,pragmatic ambiguity can be referred to as a condition where words have multiple interpretations this condition arises when the meaning of words is not specific i e it can give different meanings 
tools used training nlp models,the tools used to train nlp models are nltk spacy pytorchnlp opennlp 
models reduce dimensionality data ,the commonly used models are tfidf wordvec glove lsi topic modelling elmo embeddings 
opensource libraries nlp,the popular libraries are nltk natural language toolkit scikit learn textblob corenlp spacy gensim 
explain masked language model,masked modeling is an example of autoencoding language modeling here the output is predicted from corrupted input by this model we can predict the word from other words present in the sentences 
bag words model,the bagofwords model is used for information retrieval here the text is represented as a multiset i e a bag of words we don t consider grammar and word order but we surely maintain the multiplicity 
cbow ,cbow or continuous bag of words is a model that tries to predict the target word from the available source context words i e the surrounding words here the context words are taken into account as multiple words for a given target word 
tfidf uses,tfidf is an abbreviation for the term frequencyinverse documentary frequency it is used to provide a numeric value to a word to show how important it is in the document or a corpus 
parts speech tagging,parts of speech pos are the functions of the word like a noun verb etc and tagging is labeling the words present in the sentences into different parts of speech 
ngram,ngrams are the continuous sequence similar to the power set in number theory of ntokens of a given text 
skipgram,skip gram is an unsupervised learning technique used to find the most related words to a target word it is a reverse process of the continuous bag of words model 
corpus ,corpus or corpora plural is a collection of the text of a similar type for example movie reviews social media posts etc 
keyword normalization,keyword normalization is an nlp technique where we apply normalization on a word to condense it to its most basic form 
lemmatization ,lemmatization is a type of normalization used to group similar terms to their base formbased on the parts of speech for example talking and talking can be mapped to a single term walk 
stemming ,stemming in nlp is also a type of normalization and is similar to lemmatization but the difference here is that it segregates the words without the parts of speech tags it is faster than lemmatization and can also be more accurate in some cases 
ambiguity ,ambiguity can be referred to as a condition when a word can have multiple interpretations and results in being misunderstood natural languages are ambiguous and can make it difficult to process nlp techniques on them resulting in the wrong output 
tokenization ,tokenization is the process of breaking down large sets of text into small parts for easy readability and understanding each small part is referred to as text and provides a piece of meaningful information 
stop words ,stop words are the unwanted text that is present in the input it is the process of removal of unwanted text from further processing of text for example a to can etc 
word similarity ,word similarity in nlp is done by calculating the word vectors of the words in the vector space and then calculating the similarity on a scale of to 
sentence similarity,sentence similarity is done in nlp by finding the cosine similarity between the two sentences it can be done by finding the cosine angle between the vectors of two sentences in the inner product space 
document similarity,document similarity is done in nlp by converting the documents into the tfidf vectors form and finding their cosine similarity 
transformers,transformers are deep learning architectures that can parallelize computations they are used to learn long term dependencies 
latent semantic indexing lsi,latent semantic indexing also referred to as the latent semantic analysis is an nlp technique used to remove stop words from processing the text into the text s main content it is used to find relationships between different words 
named entity recognition ner,named entity recognition is a part of information retrieval a method to locate and classify the entities present in the unstructured data provided and convert them into predefined categories 
nltk nlp,ntlk an abbreviation of natural language toolkit is one of nlp s most popular libraries it was written in python and contained libraries for tokenization classification tagging stemming parsing and semantic reasoning 
spacy,spacy is an opensource library for natural language processing on an advanced level it is mostly used for productionlevel usage and uses convolutional neural network models 
opennlp,opennlp is a java based library used for natural language processing and it supports most of the nlp tasks such as tokenization language detection etc 
dependency parsing,dependency parsing also called syntactic parsing recognizes a dependency parse of a sentence and assigns a syntax structure to a sentence it focuses on the relationship between different words 
shallow parsing,shallow parsing also known as light parsing and chunking identifies constituents of sentences and then links them to different groups of grammatical meanings 
language modeling,language modeling is the process of creating a probability distribution of a sequence of words it is used to provide probability to all the words present in the sequence 
topic modeling,topic modeling is a method of finding abstract topics in a document or set of documents to find hidden semantic structures 
text summarisation ,text summarization in nlp is the process of conversion of large pieces of text to short text it is intended to summarize the given text keeping the main contents and overall meaning intact 
difference regular expression regular grammar,the difference between regular expression and  regular grammar is that regular grammar is used to generate regular language and regular expression is used to represent regular language 
perplexity ,perplexity is the condition when the system encounters something unaccountable or which is not meaningful 
pagerank algorithm,google uses the pagerank algorithm it is the algorithm to rank web pages in the search engine results 
noise removal,noise removal is one of the nlp techniques i e used to remove pieces of text from the corpus that is not necessary as they can hinder our text analysis 
word embedding,word embedding is the process of mapping words from the vocabulary to vectors of real numbers 
wordvec,wordvec is a collection of models that are used to generate word embeddings these models are trained to reconstruct the linguistic context of the words in the corpus 
docvec,docvec is one of the unsupervised algorithms used to generate vectors of sentences or documents irrespective of their length 
document term matrix,the document term matrix also called the termdocument matrix is the matrix that describes the frequency of terms occurring in a document 
wordnet,wordnet can be described as a database created to store words from different languages connected by their semantic relationships 
flexible string matching,flexible string matching or fuzzy string matching is a method to find strings that are likely to match a specific pattern it is also called approximate string matching as it uses an approximation to find patterns between strings 
cosine similarity,cosine similarity is the measure of cosine difference between two nonzero vectors in the inner product space it is used to find the similarity between documents irrespective of their size 
information extraction,information extraction is the process of extracting useful data in a structured way from a given unstructured set of data 
object standardization used,object standardization is the process of extracting useful information from abbreviations and other acronyms that can not be meaningful in lexical dictionaries 
text generation ,textgeneration is the process of generating natural language texts automatically in response to the communication it uses artificial intelligence and computational linguistic knowledge to perform this task 
estimate entropy english language,ngrams can estimate the entropy of the english language the entropy of a letter is calculated by knowing the entropy of all the previous letters 
latent dirichlet allocation,latent dirichlet allocation is a topic modeling method where each topic represents a set of words and every document is made of various words 
conditional random fields,conditional random fields crfs are a collection of statistical modeling methods it is used for pattern recognition and structure prediction 
hidden markov random fields,hidden markov random fields are a derivation of the hidden markov model it is a process generated by a markov chain whose state sequence can only be observed by a sequence of observations 
coreference resolution,coreference resolution is the process of collecting all the expressions that are referring to the same entity in a text it is used in information extraction document summarization and question answering 
pac learning,probably approximately correct learning is a mathematical analysis framework it is used for the analysis of generalization error of the learning algorithms 
sequence learning,sequence learning is a method of learning where both input and output are sequences 
types machine learning algorithms,different types of machine learning algorithms are supervised unsupervised and reinforcement learning 
supervised learning,supervised learning is a machine learning algorithm of inferring a function from labeled training data 
unsupervised learning,unsupervised learning is also a type of machine learning algorithm used to find patterns on the set of data given it dont have any dependent variable or label to predict 
naive bayes,the naive bayes method is a supervised learning algorithm it is naive since it makes assumptions by applying bayes theorem that all attributes are independent of each other 
pca ,principal component analysis pca is most commonly used for dimension reduction in this case pca measures the variation in each variable or column in the table pca is used in finance neuroscience and pharmacology it is very useful as a preprocessing step especially when there are linear correlations between features 
support vector machine,a support vector machine svm is an algorithm that tries to fit a line or plane or hyperplane between the different classes that maximizes the distance from the line to the points of the classes in this way it tries to find a robust separation between the classes 
bias,bias in data tells us there is inconsistency in data the inconsistency may occur for several reasons which are not mutually exclusive 
regression algoithms,regression is used to predict the relationship that data represents
classification algorithms,classification is used to classify data into some specific categories 
underfitting,underfitting case we are not able to understand or capture the patterns from the data 
overfitting ,overfitting means the model fitted to training data too well whereas for the 
neural network,it is a simplified model of the human brain much like the brain it has neurons that activate when encountering something similar 
loss function,when calculating loss we consider only a single data point then we use the term loss function
cost functions, when calculating the loss sum of error for multiple data then we use the cost function 
outlier ,an outlier is an observation in the dataset that is far away from other observations in the dataset tools used to discover outliers are box plot zscore scatter plot etc 
collaborative filtering,collaborative filtering is a proven technique for personalized content recommendations
 contentbased filtering, contentbased recommender systems are focused only on the preferences of the user 
clustering,clustering is the process of grouping a set of objects into a number of groups objects should be similar to one another within the same cluster and dissimilar to those in other clusters hierarchical clustering k means clustering 
recommendation  engine,a recommendation engine is a system used to predict users interests and recommend products that are quite likely interesting for them 
covariance, covariance is a simple way to measure the correlation between two variables he problem with covariance is that they are hard to compare without normalization 
correlation ,correlation is used for measuring and also for estimating the quantitative relationship between two variables correlation measures how strongly two variables are related
pvalue,pvalues are used to make a decision about a hypothesis test pvalue is the minimum significant level at which you can reject the null hypothesis 
parametric models,nonparametric models have no limits in taking a number of parameters allowing for more flexibility and to predict new data 
nonparametric models,parametric models will have limited parameters 
reinforcement learning,in reinforcement learning we are given neither data nor labels learning is based on the rewards given to the agent by the environment 
machine learning,machine learning is a branch of computer science which deals with system programming in order to automatically learn and improve with experience 
inductive machine learning,the inductive machine learning involves the process of learning by examples where a system from a set of observed instances tries to induce a general rule 
genetic programming,genetic programming is one of the two techniques used in machine learning the model is based on the testing and selecting the best choice among a set of results 
model selection machine learning,the process of selecting models among different mathematical models which are used to describe the same data set is known as model selection 
difference heuristic rule learning heuristics decision trees,the difference is that the heuristics for decision trees evaluate the average quality of a number of disjointed sets while rule learners only evaluate the quality of the set of instances that is covered with the candidate rule 
perceptron machine learning,perceptron is a supervised learning algorithm for binary classifiers
components bayesian logic program,bayesian logic program consists of two components the first component is a logical one and the second component is a quantitative one 
instance based learning algorithm sometimes referred lazy learning algorithm,instance based learning algorithm is also referred as lazy learning algorithm as they delay the induction or generalization process until classification is performed 
incremental learning ,incremental learning method is the ability of an algorithm to learn from new data that may be available after classifier has already been generated from already available dataset 
dimension reduction machine learning, dimension reduction is the process of reducing the number of random variables under considerations and can be divided into feature selection and feature extraction 
