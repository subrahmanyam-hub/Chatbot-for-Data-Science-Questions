{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"app.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ee73de6b"},"source":["## Importing relevant libraries"],"id":"ee73de6b"},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:32.029068Z","start_time":"2021-10-09T10:12:31.811839Z"},"id":"9906d436"},"source":["import pandas as pd\n","from flask import Flask, render_template, request\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer"],"id":"9906d436","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:51.915803Z","start_time":"2021-10-09T10:13:32.029068Z"},"id":"fd0bac21"},"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","from wordcloud import WordCloud,STOPWORDS #Better stopwords here"],"id":"fd0bac21","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:51.946599Z","start_time":"2021-10-09T10:13:51.922036Z"},"id":"e07c9389"},"source":["labels = []\n","questions = []\n","for line in open('final_quest.txt', encoding=\"utf8\"):\n","    labels.append(line.strip().split(\" \")[-1])\n","    questions.append(\" \".join(line.strip().split(\" \")[:-1]))\n","answers = []\n","for line in open('final_ans.txt', encoding=\"utf8\"):\n","    answers.append(line.strip())"],"id":"e07c9389","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:52.152390Z","start_time":"2021-10-09T10:13:51.950736Z"},"id":"cc3ed41b"},"source":["response  = answers .copy()"],"id":"cc3ed41b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:52.388154Z","start_time":"2021-10-09T10:13:52.156188Z"},"scrolled":false,"id":"2bb7057f","outputId":"0eca456c-b591-432a-a312-5f84fbeecba5"},"source":["response"],"id":"2bb7057f","execution_count":null,"outputs":[{"data":{"text/plain":["['adjusted rsquared is quite different from rsquared as it takes into account the number of independent variables that are used for predicting the target variable in this way one can determine if the addition of these new variables will actually increase the model fit or not',\n"," 'the advanced ensemble techniques are stacking blending bagging boosting',\n"," 'ambiguity can be referred to as a condition when a word can have multiple interpretations and results in being misunderstood natural languages are ambiguous and can make it difficult to process nlp techniques on them resulting in the wrong output',\n"," 'area under roc curve is a performance metric for measuring the ability of a binary classifier to discriminate between positive and negative classes',\n"," 'the bagofwords model is used for information retrieval here the text is represented as a multiset i e a bag of words we don t consider grammar and word order but we surely maintain the multiplicity',\n"," 'bagging is combining the results of multiple models for instance all decision trees to get a generalized result different bagging algorithms are bagging metaestimator random forest',\n"," 'bagging metaestimator is an ensembling algorithm that can be used for both classification baggingclassifier and regression baggingregressor problems it follows the typical bagging technique to make predictions following are the steps for the bagging metaestimator algorithm',\n"," 'bias in data tells us there is inconsistency in data the inconsistency may occur for several reasons which are not mutually exclusive',\n"," 'blending uses only a holdout validation set from the train set to make predictions',\n"," 'boosting is a sequential process where each subsequent model attempts to correct the errors of the previous model the succeeding models are dependent on the previous model different boosting algorithms are adaboost gbm xgbm light gbm catboost',\n"," 'bootstrapping is a sampling technique in which we create subsets of observations from the original dataset with replacement the size of the subsets is the same as the size of the original set',\n"," 'catboost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithm',\n"," 'cbow or continuous bag of words is a model that tries to predict the target word from the available source context words i e the surrounding words here the context words are taken into account as multiple words for a given target word',\n"," 'classification accuracy is a common evaluation metric for classification problems it is the number of correct predictions made as a ratio of all predictions made we use sklearn module to compute the accuracy of a classification task',\n"," 'classification is used to classify data into some specific categories',\n"," 'clustering is the process of grouping a set of objects into a number of groups objects should be similar to one another within the same cluster and dissimilar to those in other clusters hierarchical clustering k means clustering',\n"," 'collaborative filtering is a proven technique for personalized content recommendations',\n"," 'bayesian logic program consists of two components the first component is a logical one and the second component is a quantitative one',\n"," 'conditional random fields crfs are a collection of statistical modeling methods it is used for pattern recognition and structure prediction',\n"," 'confidence interval or ci is the range of values which is required to meet a certain confidence level in order to estimate the features of the total population',\n"," 'a confusion matrix can be defined as the n n matrix that is used to evaluate the performance of a classification model n is the number of target classes the matrix is used to evaluate and compare the actual target values with that of the values predicted by machine model learning',\n"," 'contentbased recommender systems are focused only on the preferences of the user',\n"," 'coreference resolution is the process of collecting all the expressions that are referring to the same entity in a text it is used in information extraction document summarization and question answering',\n"," 'corpus or corpora plural is a collection of the text of a similar type for example movie reviews social media posts etc',\n"," 'correlation is used for measuring and also for estimating the quantitative relationship between two variables correlation measures how strongly two variables are related',\n"," 'correlation analysis is a statistical method to evaluate the strength of the relationship between two quantitative variables it consists of autocorrelation coefficients estimated and calculated to make a different spatial relationship it is used to correlate data based on distance',\n"," 'cosine similarity is the measure of cosine difference between two nonzero vectors in the inner product space it is used to find the similarity between documents irrespective of their size',\n"," 'when calculating the loss sum of error for multiple data then we use the cost function',\n"," 'covariance is a simple way to measure the correlation between two variables he problem with covariance is that they are hard to compare without normalization',\n"," 'crossvalidation is a technique that involves partitioning the original observation dataset into a training set used to train the model and an independent set used to evaluate the analysis',\n"," 'dependency parsing also called syntactic parsing recognizes a dependency parse of a sentence and assigns a syntax structure to a sentence it focuses on the relationship between different words',\n"," 'the difference is that the heuristics for decision trees evaluate the average quality of a number of disjointed sets while rule learners only evaluate the quality of the set of instances that is covered with the candidate rule',\n"," 'the difference between regular expression and regular grammar is that regular grammar is used to generate regular language and regular expression is used to represent regular language',\n"," 'different parameters used in adaboost base estimators n estimators learning rate max depth',\n"," 'different parameters used in bagging base estimator n estimators max samples max features n jobs',\n"," 'different parameters used in catboost loss function iterations learning rate',\n"," 'different parameters used in gradient boosting min samples leaf min weight fraction leaf',\n"," 'different parameters used in light gbm num iterations num leaves min data in leaf',\n"," 'different parameters used in random forest n estimators criterion max features max depth min samples split min samples leaf max leaf nodes n jobs',\n"," 'different parameters used in xgboost nthread gamma eta min child weight',\n"," 'dimension reduction is the process of reducing the number of random variables under considerations and can be divided into feature selection and feature extraction',\n"," 'discourse is a structured group of the sentence discourse analysis can be termed as an approach to analyzing the discourse i e text or language it involves text interpretations and interactions',\n"," 'document similarity is done in nlp by converting the documents into the tfidf vectors form and finding their cosine similarity',\n"," 'the document term matrix also called the termdocument matrix is the matrix that describes the frequency of terms occurring in a document',\n"," 'docvec is one of the unsupervised algorithms used to generate vectors of sentences or documents irrespective of their length',\n"," 'ensemble learning methods is a machine learning method that contributes to combining base models to create one efficient predictive model it boosts the overall development of the process',\n"," 'ngrams can estimate the entropy of the english language the entropy of a letter is calculated by knowing the entropy of all the previous letters',\n"," 'masked modeling is an example of autoencoding language modeling here the output is predicted from corrupted input by this model we can predict the word from other words present in the sentences',\n"," 'the fscore can be determined as the harmonic mean of both precision and recall the highest value of fscore can be and the lowest can be',\n"," 'flexible string matching or fuzzy string matching is a method to find strings that are likely to match a specific pattern it is also called approximate string matching as it uses an approximation to find patterns between strings',\n"," 'genetic programming is one of the two techniques used in machine learning the model is based on the testing and selecting the best choice among a set of results',\n"," 'the gini coefficient or gini index is a popular metric for imbalanced class values it is a statistical measure of distribution developed by the italian statistician corrado gini in',\n"," 'gradient boosting or gbm is another ensemble machine learning algorithm that works for both regression and classification problems gbm uses the boosting technique combining a number of weak learners to form a strong learner regression trees used as a base learner each subsequent tree in series is built on the errors calculated by the previous tree',\n"," 'hidden markov random fields are a derivation of the hidden markov model it is a process generated by a markov chain whose state sequence can only be observed by a sequence of observations',\n"," 'incremental learning method is the ability of an algorithm to learn from new data that may be available after classifier has already been generated from already available dataset',\n"," 'the inductive machine learning involves the process of learning by examples where a system from a set of observed instances tries to induce a general rule',\n"," 'information extraction is the process of extracting useful data in a structured way from a given unstructured set of data',\n"," 'instance based learning algorithm is also referred as lazy learning algorithm as they delay the induction or generalization process until classification is performed',\n"," 'keyword normalization is an nlp technique where we apply normalization on a word to condense it to its most basic form',\n"," 'in kfold crossvalidation we divide the dataset into k equal parts after this we loop over the entire dataset k times in each iteration of the loop one of the k parts is used for testing and the other k parts are used for training using kfold crossvalidation each one of the k parts of the dataset ends up being used for training and testing purposes',\n"," 'language modeling is the process of creating a probability distribution of a sequence of words it is used to provide probability to all the words present in the sequence',\n"," 'latent dirichlet allocation is a topic modeling method where each topic represents a set of words and every document is made of various words',\n"," 'latent semantic indexing also referred to as the latent semantic analysis is an nlp technique used to remove stop words from processing the text into the text s main content it is used to find relationships between different words',\n"," 'lemmatization is a type of normalization used to group similar terms to their base formbased on the parts of speech for example talking and talking can be mapped to a single term walk',\n"," 'lightgbm is a gradient boosting framework that uses treebased algorithms and follows leafwise approach while other algorithms work in a levelwise approach pattern the images below will help you understand the difference in a better way',\n"," 'logarithmic loss logloss measures the performance of a classification model where the prediction input is a probability value between and log loss increases as the predicted probability diverges from the actual label the goal of machine learning models is to minimize this value as such smaller logloss is better with a perfect model having a log loss of',\n"," 'when calculating loss we consider only a single data point then we use the term loss function',\n"," 'machine learning is a branch of computer science which deals with system programming in order to automatically learn and improve with experience',\n"," 'the max voting method is generally used for classification problems in this technique multiple models are used to make predictions for each data point the predictions by each model are considered as a vote the predictions which we get from the majority of the models are used as the final prediction',\n"," 'mean absolute error can be defined as the difference between the actual values and the predicted values s',\n"," 'mean squared error or commonly known as the mse is the average of the squared errors used as the loss function for the least squares values of regression it is basically the sum of the square of the difference between the predicted target variables and the actual target variables which is finally divided by the number of data points',\n"," 'model evaluation metrics are required to quantify model performance the choice of evaluation metrics depends on a given machine learning task such as classification regression ranking clustering topic modeling among others some metrics such as precisionrecall are useful for multiple tasks supervised learning tasks such as classification and regression constitutes a majority of machine learning applications in this article we focus on metrics for these two supervised learning models',\n"," 'the process of selecting models among different mathematical models which are used to describe the same data set is known as model selection',\n"," 'the commonly used models are tfidf wordvec glove lsi topic modelling elmo embeddings',\n"," 'the naive bayes method is a supervised learning algorithm it is naive since it makes assumptions by applying bayes theorem that all attributes are independent of each other',\n"," 'named entity recognition is a part of information retrieval a method to locate and classify the entities present in the unstructured data provided and convert them into predefined categories',\n"," 'natural language generation is a part of ai and generates natural language texts from structured data to produce an output it can be seen as nlp s reverse process where nlp is used to understand and interpret the natural language to form data and nlu is used to generate outputs in natural language from structured data',\n"," 'it is a simplified model of the human brain much like the brain it has neurons that activate when encountering something similar',\n"," 'ngrams are the continuous sequence similar to the power set in number theory of ntokens of a given text',\n"," 'ntlk an abbreviation of natural language toolkit is one of nlp s most popular libraries it was written in python and contained libraries for tokenization classification tagging stemming parsing and semantic reasoning',\n"," 'noise removal is one of the nlp techniques i e used to remove pieces of text from the corpus that is not necessary as they can hinder our text analysis',\n"," 'parametric models will have limited parameters',\n"," 'object standardization is the process of extracting useful information from abbreviations and other acronyms that can not be meaningful in lexical dictionaries',\n"," 'opennlp is a java based library used for natural language processing and it supports most of the nlp tasks such as tokenization language detection etc',\n"," 'the popular libraries are nltk natural language toolkit scikit learn textblob corenlp spacy gensim',\n"," 'an outlier is an observation in the dataset that is far away from other observations in the dataset tools used to discover outliers are box plot zscore scatter plot etc',\n"," 'overfitting means the model fitted to training data too well whereas for the',\n"," 'probably approximately correct learning is a mathematical analysis framework it is used for the analysis of generalization error of the learning algorithms',\n"," 'google uses the pagerank algorithm it is the algorithm to rank web pages in the search engine results',\n"," 'nonparametric models have no limits in taking a number of parameters allowing for more flexibility and to predict new data',\n"," 'parts of speech pos are the functions of the word like a noun verb etc and tagging is labeling the words present in the sentences into different parts of speech',\n"," 'principal component analysis pca is most commonly used for dimension reduction in this case pca measures the variation in each variable or column in the table pca is used in finance neuroscience and pharmacology it is very useful as a preprocessing step especially when there are linear correlations between features',\n"," 'perceptron is a supervised learning algorithm for binary classifiers',\n"," 'the performance measure is the way you want to evaluate a solution to the problem it is the measurement you will make of the predictions made by a trained model on the test dataset performance measures are typically specialized to the class of problem you are working with for example classification regression and clustering',\n"," 'perplexity is the condition when the system encounters something unaccountable or which is not meaningful',\n"," 'pragmatic ambiguity can be referred to as a condition where words have multiple interpretations this condition arises when the meaning of words is not specific i e it can give different meanings',\n"," 'the pragmatic analysis is the process of information extraction from the given text it is a set of linguistic and logical tools that enable us to churn out the meaning of the given structure of a text',\n"," 'precision is the ratio of true positives and all the positives precision gives us the measure of all the relevant data points recall is termed as the measure of the machine model that correctly identifies the true positives recall gives an estimate of how accurately your model identifies the relevant data',\n"," 'predictive power is a synthetic metric which satisfies interesting properties like it is always been and where represents that the feature subset has no predictive power and represents that the feature subset has maximum predictive power and is used to select a good subset of features in any machine learning project',\n"," 'pvalues are used to make a decision about a hypothesis test pvalue is the minimum significant level at which you can reject the null hypothesis',\n"," 'random forest is another ensemble machine learning algorithm that follows the bagging technique it is an extension of the bagging estimator algorithm the base estimators in random forest are decision trees',\n"," 'a recommendation engine is a system used to predict users interests and recommend products that are quite likely interesting for them',\n"," 'regression is used to predict the relationship that data represents',\n"," 'in reinforcement learning we are given neither data nor labels learning is based on the rewards given to the agent by the environment',\n"," 'resampling methods are used to estimate the precision of the sample statistics exchanging labels on data points and validating models',\n"," 'root mean squared erro or rmse is defined as the measure of the differences between the values predicted by a model and the values actually observed it is basically the square root of mse mean squared error which is the average of the squared error used as the loss function for least squares regression',\n"," 'rsquared can be defined as the statistical measure that is used to represent the goodness fit of a regression model the ideal value of rsquare is known to be closer the value of rsquare to better is the fitted model',\n"," 'the semantic analysis is the process of understanding the meaning of the text in the way humans perceive and communicate it focuses on larger parts of data for processing as compared to other analysis techniques',\n"," 'sentence similarity is done in nlp by finding the cosine similarity between the two sentences it can be done by finding the cosine angle between the vectors of two sentences in the inner product space',\n"," 'the sentiment analysis also known as opinion mining and emotion ai is a process of detecting the polarity of the opinion in the text or can be a part of it it is majorly used to identify extract and quantify user or customer reviews polarity survey responses or social media opinions',\n"," 'sequence learning is a method of learning where both input and output are sequences',\n"," 'shallow parsing also known as light parsing and chunking identifies constituents of sentences and then links them to different groups of grammatical meanings',\n"," 'signal processing is a method that enables software to analyze modify and synthesize signals in nlp these can be sound or text signals',\n"," 'the simple ensemble techniques are max voting avearge weighted average',\n"," 'skip gram is an unsupervised learning technique used to find the most related words to a target word it is a reverse process of the continuous bag of words model',\n"," 'spacy is an opensource library for natural language processing on an advanced level it is mostly used for productionlevel usage and uses convolutional neural network models',\n"," 'stacking is an ensemble learning technique that uses predictions from multiple models for example decision tree knn or svm to build a new model this model is used for making predictions on the test set below is a stepwise explanation for a simple stacked ensemble',\n"," 'stemming in nlp is also a type of normalization and is similar to lemmatization but the difference here is that it segregates the words without the parts of speech tags it is faster than lemmatization and can also be more accurate in some cases',\n"," 'stop words are the unwanted text that is present in the input it is the process of removal of unwanted text from further processing of text for example a to can etc',\n"," 'supervised learning is a machine learning algorithm of inferring a function from labeled training data',\n"," 'a support vector machine svm is an algorithm that tries to fit a line or plane or hyperplane between the different classes that maximizes the distance from the line to the points of the classes in this way it tries to find a robust separation between the classes',\n"," 'the syntactic analysis also referred to as parsing and syntax analysis is the phase in which we try to process the given text s structure this process tries to draw meaning from the text by comparing it to formal grammar rules or syntax',\n"," 'the test harness is the data you will train and test an algorithm against and the performance measure you will use to assess its performance it is important to define your test harness well so that you can focus on evaluating different algorithms and thinking deeply about the problem',\n"," 'textgeneration is the process of generating natural language texts automatically in response to the communication it uses artificial intelligence and computational linguistic knowledge to perform this task',\n"," 'text summarization in nlp is the process of conversion of large pieces of text to short text it is intended to summarize the given text keeping the main contents and overall meaning intact',\n"," 'tfidf is an abbreviation for the term frequencyinverse documentary frequency it is used to provide a numeric value to a word to show how important it is in the document or a corpus',\n"," 'tokenization is the process of breaking down large sets of text into small parts for easy readability and understanding each small part is referred to as text and provides a piece of meaningful information',\n"," 'the tools used to train nlp models are nltk spacy pytorchnlp opennlp',\n"," 'topic modeling is a method of finding abstract topics in a document or set of documents to find hidden semantic structures',\n"," 'transformers are deep learning architectures that can parallelize computations they are used to learn long term dependencies',\n"," 'type i error false positive error it occurs when the null hypothesis is true but it gets rejected meaning it claims something has happened when it hasn t t',\n"," 'ype ii error false negative error it occurs when the null hypothesis gets accepted when it s not true meaning it claims nothing happened when something has happened',\n"," 'different types of machine learning algorithms are supervised unsupervised and reinforcement learning',\n"," 'underfitting case we are not able to understand or capture the patterns from the data',\n"," 'unsupervised learning is also a type of machine learning algorithm used to find patterns on the set of data given it dont have any dependent variable or label to predict',\n"," 'voting classifiers are usually used in classification problems imagine you trained and fitted a few classifiers logistic regression classifier svm classifiers random forest classifier among others to your training dataset',\n"," 'weighted average is an extension of the averaging method all models are assigned different weights defining the importance of each model for prediction for instance if two of your colleagues are critics while others have no prior experience in this field then the answers by these two friends are given more importance as compared to the other people',\n"," 'word embedding is the process of mapping words from the vocabulary to vectors of real numbers',\n"," 'word similarity in nlp is done by calculating the word vectors of the words in the vector space and then calculating the similarity on a scale of to',\n"," 'wordnet can be described as a database created to store words from different languages connected by their semantic relationships',\n"," 'wordvec is a collection of models that are used to generate word embeddings these models are trained to reconstruct the linguistic context of the words in the corpus',\n"," 'xgboost extreme gradient boosting is an advanced implementation of the gradient boosting algorithm xgboost has proved to be a highly effective ml algorithm extensively used in machine learning competitions and hackathons xgboost has high predictive power and is almost times faster than the other gradient boosting techniques it also includes a variety of regularization which reduces overfitting and improves overall performance',\n"," 'Hi! I am your data science interview chat bot. please ask your interview questions.',\n"," 'Okay, Please take care. see you later.']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:52.637327Z","start_time":"2021-10-09T10:13:52.390104Z"},"id":"213aae4d"},"source":["# using bag of words- vectorising the keywords"],"id":"213aae4d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:52.747093Z","start_time":"2021-10-09T10:13:52.641831Z"},"id":"3601b660","outputId":"75d145d5-f71a-4292-d9d7-6564b7f1e4b0"},"source":["bow_vectorizer = CountVectorizer(stop_words = STOPWORDS,lowercase = True, ngram_range = (1,2))\n","training_vectors = bow_vectorizer.fit_transform(questions)"],"id":"3601b660","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\anoop\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n","  warnings.warn('Your stop_words may be inconsistent with '\n"]}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:52.778214Z","start_time":"2021-10-09T10:13:52.751028Z"},"id":"af3924ed"},"source":["# Naive Bayes Model"],"id":"af3924ed","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:52.966300Z","start_time":"2021-10-09T10:13:52.779960Z"},"id":"91b2e076","outputId":"c798656a-ee0e-495d-fea3-a93cf1026ae6"},"source":["classifier = MultinomialNB()\n","classifier.fit(training_vectors, labels)"],"id":"91b2e076","execution_count":null,"outputs":[{"data":{"text/plain":["MultinomialNB()"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-09T10:13:52.982216Z","start_time":"2021-10-09T10:13:52.970174Z"},"id":"a6f07914"},"source":["# Deployment in Flask"],"id":"a6f07914","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"start_time":"2021-10-09T10:12:31.827Z"},"id":"1d307b53","outputId":"e1747294-2d2c-4dd7-86b6-a96083dad11f"},"source":["app = Flask(__name__)\n","@app.route(\"/\")\n","def home():\n","    return render_template(\"index.html\")\n","\n","@app.route(\"/get\")\n","def get_bot_response():\n","    user_request = request.args.get('msg')  # Fetching input from the user\n","    input_vector = bow_vectorizer.transform([user_request])\n","    predict = classifier.predict(input_vector)\n","    index = int(predict[0])\n","    return response[index-1]\n","    \n","\n","if __name__ == \"__main__\":\n","    app.run(threaded=False)"],"id":"1d307b53","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","   WARNING: This is a development server. Do not use it in a production deployment.\n","   Use a production WSGI server instead.\n"," * Debug mode: off\n"]},{"name":"stderr","output_type":"stream","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n","127.0.0.1 - - [09/Oct/2021 15:44:53] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [09/Oct/2021 15:44:53] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [09/Oct/2021 15:44:53] \"\u001b[36mGET /static/style.css HTTP/1.1\u001b[0m\" 304 -\n","127.0.0.1 - - [09/Oct/2021 15:44:53] \"\u001b[36mGET /static/image_opacity.jpg HTTP/1.1\u001b[0m\" 304 -\n","127.0.0.1 - - [09/Oct/2021 15:44:53] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [09/Oct/2021 15:44:58] \"\u001b[37mGET /get?msg=hi HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [09/Oct/2021 15:45:06] \"\u001b[37mGET /get?msg=good%20bye HTTP/1.1\u001b[0m\" 200 -\n"]}]}]}